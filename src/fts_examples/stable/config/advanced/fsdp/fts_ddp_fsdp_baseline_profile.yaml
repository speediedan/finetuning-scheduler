trainer:
  limit_train_batches: 7
  max_epochs: 4
  devices: 2
  callbacks:
  - class_path: finetuning_scheduler.FinetuningScheduler
    init_args:
      ft_schedule: ./config/RteBoolqModule_ft_schedule_deberta_base_fsdp.yaml
      max_depth: 2
  - class_path: finetuning_scheduler.FTSCheckpoint
    init_args:
      save_top_k: 1
      monitor: val_loss
      verbose: true
  - class_path: finetuning_scheduler.FTSEarlyStopping
    init_args:
      monitor: val_loss
      min_delta: 0.001
      patience: 2 # limited patience for example
      verbose: false
      mode: min
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: lightning_logs
      name: fts_ddp_fsdp_baseline_profile
  profiler:
    class_path: fts_examples.stable.extended_profiler.ExtendedPyTorchProfiler
    init_args:
      filename: fts_fsdp_awp_text_profile
      max_name_column_width: 100
      sort_by_key: cuda_time_total
      schedule_cfg:
        # note, there is currently (as of 2023.04.15) an open issue regarding superfluous profiler messages
        # https://github.com/pytorch/pytorch/issues/91886
        skip_first: 20  # comment if you want to profile the first fine-tuning phase instead of the final one
        wait: 1
        warmup: 1
        active: 3
    dict_kwargs:
      with_stack: true
      profile_memory: true
      record_shapes: true
      row_limit: 50
