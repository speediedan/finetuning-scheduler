trainer:
  max_epochs: 4
  callbacks+:
  - class_path: finetuning_scheduler.FinetuningScheduler
    init_args:
      ft_schedule: ./config/defaults/llama_ft_schedule.yaml
      max_depth: 2
      strategy_adapter_cfg:
        fsdp_default_kwargs:
          act_ckpt: ['composable']
          # cpu_offload_policy: {}
        fsdp_plan: {'model.output': {}, 'model.layers.\d*$': {}, 'model': {}}
  strategy:
    init_args:
      data_parallel_size: 2
      tensor_parallel_size: 1
  logger:
    init_args:
      name: fts_fsdp_profiling
